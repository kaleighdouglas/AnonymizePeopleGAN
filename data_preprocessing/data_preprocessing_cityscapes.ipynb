{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for PS-GAN using cityscapes dataset\n",
    "\n",
    "Input: The code requires a set of images from cityscapes dataset and the corresponding json files with bounding box pedestrian annotations  \n",
    "\n",
    "Output: For each bounding box pedestrian annotation associated with an image, the bounding box region is replaced with noise and a cropped 256 x 256 image centered around the noisy region is created and saved to directory A.  The corresponding cropped region of the original image is also saved to directory B. The bounding box annotations are saved to a json file in the format required by PS-GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "- Noise is created with random black, white, grey pixels in equal probabilities\n",
    "- Only includes cropped images where bounding box height is between 28 and 250 and bounding box width is between 28 and 250. (The smaller bounds of 28 differ from the paper, which specifies 70 min height and 25 min width, and the larger bound of 250 is our additional restriction).\n",
    "- Paper seems to use bbox, however we use bboxVis\n",
    "- Train/val/test data is all taken from all cities (they are not separated by city as in the orginal cityscapes dataset)\n",
    "\n",
    "To Do:\n",
    "- Figure out a way to include people who are > 250 pixels wide/tall\n",
    "- Ensure that people who are <28 pixels are too small to be recognized, else figure out how to include them\n",
    "- Include people who are annotated as \"riders\" (cyclists, wheelchair users, etc)\n",
    "- Change bboxes_AB json format from {x,y,w,h} to {x1,y1,x2,y2} when code is changed\n",
    "- Write explanation of how train/val/test set was computed\n",
    "- Write explanation of directories [A,B,AB,bboxes_AB,images,bboxes]\n",
    "- Write explanation of how to process data start to finish\n",
    "\n",
    "\n",
    "Code:\n",
    "- specify the depth of the U-net\n",
    "\n",
    "\n",
    "## Original Images  \n",
    "Training Dataset:\n",
    "- Total number of input images: 2975\n",
    "- Total number of people saved as cropped output images: 8191\n",
    "- Total number of people annotations in input data: 16526\n",
    "- Percent people included: 0.5\n",
    "\n",
    "Validation Dataset:\n",
    "- 500 Images\n",
    "\n",
    "Cityscapes Test Dataset:\n",
    "- 1525 Images\n",
    "- Bounding Box annotations not provided\n",
    "\n",
    "\n",
    "## Cropped Images\n",
    "Dataset splits (cropped images - using w>= 25 and h>=70):  \n",
    "- training data: 5383  \n",
    "- validation data: 1042  \n",
    "- test data: 1168  \n",
    "\n",
    "Dataset splits 'split_w28h28' (cropped images - using w>= 28 and h>=28):\n",
    "- training data: 5667\n",
    "- validation data: 1063\n",
    "- test data: 1235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8GA3cbgkuxw4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "IMAGE_COUNT = 0 # Total number of input images processed\n",
    "TOTAL_PEOPLE = 0 # Total number of people annotated in dataset\n",
    "INCLUDED_PEOPLE = 0 # Total number of people saved as cropped image\n",
    "\n",
    "IMG_WIDTH = 2048\n",
    "IMG_HEIGHT = 1024\n",
    "\n",
    "TRAIN = 0\n",
    "VAL = 0\n",
    "TEST = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Training Dataset\n",
    "with open('train_images.pkl', 'rb') as f:\n",
    "    train_images = pickle.load(f)\n",
    "\n",
    "## Load Validation Dataset\n",
    "with open('val_images.pkl', 'rb') as f:\n",
    "    val_images = pickle.load(f)\n",
    "\n",
    "## Load Test Dataset\n",
    "with open('test_images.pkl', 'rb') as f:\n",
    "    test_images = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_directory(directory_input, directory_output, split=True):\n",
    "    global IMAGE_COUNT\n",
    "    global TRAIN\n",
    "    global VAL\n",
    "    global TEST\n",
    "    \n",
    "    images = os.listdir('images/'+directory_input)\n",
    "    if '.DS_Store' in images:\n",
    "        images.remove('.DS_Store')\n",
    "#     print(images)\n",
    "    IMAGE_COUNT += len(images)\n",
    "\n",
    "    for image_path in images:\n",
    "        if split:\n",
    "            ## Determine train/val/test\n",
    "            if image_path in train_images:\n",
    "                directory_output_img = 'train_' + directory_output\n",
    "                TRAIN += 1\n",
    "            elif image_path in val_images:\n",
    "                directory_output_img = 'val_' + directory_output\n",
    "                VAL += 1\n",
    "            elif image_path in test_images:\n",
    "                directory_output_img = 'test_' + directory_output\n",
    "                TEST += 1\n",
    "            else:\n",
    "                print('ERROR file not found', image_path)\n",
    "                raise  \n",
    "        else:\n",
    "            directory_output_img = directory_output\n",
    "        \n",
    "        ## Get Filename\n",
    "        filename = image_path[:-16]\n",
    "\n",
    "        ## Read Image\n",
    "        img = cv2.imread('images/'+directory_input+'/'+image_path)\n",
    "        assert img.shape == (1024, 2048, 3)\n",
    "\n",
    "#         ## Display Image\n",
    "#         img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting BGR to RGB for display\n",
    "#         display(Image.fromarray(img_display))\n",
    "\n",
    "\n",
    "        ## Get Bounding Box Annotations\n",
    "        bbox_file = 'bboxes/'+directory_input+'/'+filename+'_gtBboxCityPersons.json'\n",
    "\n",
    "        bbox_list = []\n",
    "        with open(bbox_file) as f:\n",
    "            data = json.load(f)\n",
    "            for obj in data['objects']:\n",
    "                if obj['label'] == 'pedestrian':\n",
    "#                     print(obj)\n",
    "                    bbox_list.append(obj['bboxVis'])\n",
    "                    \n",
    "        process_single_image(img, bbox_list, directory_output_img, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(img, bbox_list, directory_output, filename='img'):\n",
    "    global TOTAL_PEOPLE\n",
    "    \n",
    "    for i, bbox in enumerate(bbox_list):\n",
    "        TOTAL_PEOPLE += 1\n",
    "        \n",
    "        x, y, w, h = bbox   ## x,y is top left coordinate in bbox\n",
    "        \n",
    "        ## handle annotations mistake - if bounding box goes past right edge, shift x to fit\n",
    "        pixels_past_right_edge = x + w - IMG_WIDTH  \n",
    "        if pixels_past_right_edge > 0:\n",
    "            x = x - pixels_past_right_edge - 1\n",
    "            \n",
    "        ## handle annotations mistake - if bounding box goes past bottom edge, shift y to fit\n",
    "        pixels_past_bottom_edge = y + h - IMG_HEIGHT \n",
    "        if pixels_past_bottom_edge > 0:\n",
    "            y = y - pixels_past_bottom_edge - 1\n",
    "        \n",
    "        ## Create filename for cropped img\n",
    "        img_path = directory_output+'/'+filename+'_'+str(i)\n",
    "        \n",
    "        ## Only process if person is not too small/large within img\n",
    "        if w >= 28 and w <= 250 and h >= 28 and h <= 250:\n",
    "            replace_person_with_noise(img_path, img, x, x+w, y, y+h, w, h)\n",
    "        \n",
    "    return\n",
    "    \n",
    "# process_single_image(img, bbox_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bbox_json(x, y, w, h, img_path):\n",
    "    bbox_dict = {'x':x, 'y':y, 'w':x+w+1, 'h':y+h+1}\n",
    "    with open('bboxes_AB/'+img_path+'.json', 'w') as f:\n",
    "        json.dump(bbox_dict, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine offsets (where to crop image wrt noisy patch)  \n",
    "def compute_cropped_offsets(x_mid, y_mid):\n",
    "    if x_mid < 128:\n",
    "        x_left_offset =  x_mid\n",
    "        x_right_offset = 256 - x_left_offset\n",
    "    elif x_mid > (IMG_WIDTH - 128):\n",
    "        x_right_offset =  (IMG_WIDTH - x_mid)\n",
    "        x_left_offset = 256 - x_right_offset\n",
    "    else:\n",
    "        x_left_offset = 128\n",
    "        x_right_offset = 256 - x_left_offset\n",
    "    \n",
    "    if y_mid < 128:\n",
    "        y_top_offset =  y_mid\n",
    "        y_bottom_offset = 256 - y_top_offset\n",
    "    elif y_mid > (IMG_HEIGHT - 128):\n",
    "        y_bottom_offset = (IMG_HEIGHT - y_mid)\n",
    "        y_top_offset = 256 - y_bottom_offset\n",
    "    else:\n",
    "        y_top_offset = 128\n",
    "        y_bottom_offset = 256 - y_top_offset\n",
    "        \n",
    "    return x_left_offset, x_right_offset, y_top_offset, y_bottom_offset\n",
    "\n",
    "\n",
    "## Crop images (returns cropped original and noisy images with new x and y coordinates)\n",
    "def crop_images(img, img_noisy, x_left, y_top, w, h):\n",
    "    ## Find bbox mid point coordinates (coords on full-sized image)\n",
    "    x_mid = x_left + (w // 2)\n",
    "    y_mid = y_top + (h // 2)\n",
    "    \n",
    "    ## Determine offsets (where to crop image wrt noisy patch)\n",
    "    x_left_offset, x_right_offset, y_top_offset, y_bottom_offset = compute_cropped_offsets(x_mid, y_mid)\n",
    "        \n",
    "    ## Crop original and noisy images around annotated person  \n",
    "    cropped_img = img[y_mid-y_top_offset:y_mid+y_bottom_offset, x_mid-x_left_offset:x_mid+x_right_offset, :]\n",
    "    cropped_noisy_img = img_noisy[y_mid-y_top_offset:y_mid+y_bottom_offset, x_mid-x_left_offset:x_mid+x_right_offset, :]\n",
    "    \n",
    "    ## Compute new X,Y coordinates after cropping image\n",
    "    new_x = x_left - (x_mid - x_left_offset)\n",
    "    new_y = y_top - (y_mid - y_top_offset)\n",
    "    \n",
    "    return cropped_img, cropped_noisy_img, new_x, new_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_person_with_noise(img_path, img, x_left, x_right, y_top, y_bottom, w, h):\n",
    "    global INCLUDED_PEOPLE\n",
    "#     print()\n",
    "#     print(img_path)\n",
    "\n",
    "    img_noisy = img.copy()\n",
    "    \n",
    "    ## Create noise patch in Black & White & Grey\n",
    "    randnoise_bw = np.random.choice([0,127,255], img_noisy[y_top:y_bottom+1, x_left:x_right+1, :].shape[:2])\n",
    "    randnoise_bw = np.dstack((randnoise_bw,randnoise_bw,randnoise_bw))\n",
    "    \n",
    "    ## Add noise patch to image\n",
    "    img_noisy[y_top:y_bottom+1, x_left:x_right+1, :] = randnoise_bw\n",
    "\n",
    "    ## Crop images (returns cropped original and noisy images with new x and y coordinates)\n",
    "    cropped_img, cropped_noisy_img, new_x, new_y = crop_images(img, img_noisy, x_left, y_top, w, h)\n",
    "\n",
    "        \n",
    "    ## Save/Display Final Cropped Images\n",
    "    if cropped_img.shape == (256,256,3):\n",
    "        # Save Images\n",
    "        cv2.imwrite('A/'+img_path+'.png', cropped_img)\n",
    "        cv2.imwrite('B/'+img_path+'.png', cropped_noisy_img)\n",
    "        \n",
    "        ## Save bbox json with new x,y coordinates\n",
    "        save_bbox_json(new_x, new_y, w, h, img_path)\n",
    "\n",
    "        INCLUDED_PEOPLE += 1\n",
    "\n",
    "#         ## Display Image\n",
    "#         display(Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)))\n",
    "#         display(Image.fromarray(cv2.cvtColor(cropped_noisy_img, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "    else:\n",
    "        print(img_path)\n",
    "        print('skip image -- cropped_img.shape', cropped_img.shape, 'w', w, 'h', h)\n",
    "        try:\n",
    "            display(Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)))\n",
    "            display(Image.fromarray(cv2.cvtColor(cropped_noisy_img, cv2.COLOR_BGR2RGB)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(output_directory, split=True):\n",
    "    if split:\n",
    "        if not os.path.exists('A/train_'+output_directory):\n",
    "            os.makedirs('A/train_'+output_directory)\n",
    "            os.makedirs('B/train_'+output_directory)\n",
    "            os.makedirs('AB/train_'+output_directory)\n",
    "            os.makedirs('A/val_'+output_directory)\n",
    "            os.makedirs('B/val_'+output_directory)\n",
    "            os.makedirs('AB/val_'+output_directory)\n",
    "            os.makedirs('A/test_'+output_directory)\n",
    "            os.makedirs('B/test_'+output_directory)\n",
    "            os.makedirs('AB/test_'+output_directory)\n",
    "        if not os.path.exists('bboxes_AB/train_'+output_directory):\n",
    "            os.makedirs('bboxes_AB/train_'+output_directory)\n",
    "            os.makedirs('bboxes_AB/val_'+output_directory)\n",
    "            os.makedirs('bboxes_AB/test_'+output_directory)\n",
    "    else:\n",
    "        if not os.path.exists('A/'+output_directory):\n",
    "            os.makedirs('A/'+output_directory)\n",
    "            os.makedirs('B/'+output_directory)\n",
    "            os.makedirs('AB/'+output_directory)\n",
    "        if not os.path.exists('bboxes_AB/'+output_directory):\n",
    "            os.makedirs('bboxes_AB/'+output_directory)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example - Single Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ COMPLETE ------\n"
     ]
    }
   ],
   "source": [
    "IMAGE_COUNT = 0\n",
    "INCLUDED_PEOPLE = 0\n",
    "TOTAL_PEOPLE = 0\n",
    "\n",
    "example_directory = 'demo'\n",
    "example_directory_output = 'demo'\n",
    "split = False\n",
    "\n",
    "create_directories(example_directory_output, split)\n",
    "            \n",
    "process_image_directory(example_directory, example_directory_output, split)\n",
    "\n",
    "print()\n",
    "print('------ COMPLETE ------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of people saved as cropped output images: 6\n",
      "Total number of people annotations in input data: 11\n",
      "Percent people included: 0.55\n"
     ]
    }
   ],
   "source": [
    "print('Total number of people saved as cropped output images:', INCLUDED_PEOPLE)\n",
    "print('Total number of people annotations in input data:', TOTAL_PEOPLE)\n",
    "print('Percent people included:', round(INCLUDED_PEOPLE/TOTAL_PEOPLE,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of people saved as cropped output images: 1311\n",
    "Total number of people annotations in input data: 4096\n",
    "Percent people included: 0.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_COUNT = 0\n",
    "INCLUDED_PEOPLE = 0\n",
    "TOTAL_PEOPLE = 0\n",
    "\n",
    "ignore_directories = ['.DS_Store', 'demo', 'test']\n",
    "\n",
    "## Specify directory name to save output files\n",
    "output_directory = 'split_w28h28_v3'\n",
    "\n",
    "## Specify whether to split into train/val/test\n",
    "split = True\n",
    "\n",
    "## Create directories to save output files\n",
    "create_directories(output_directory, split)\n",
    "            \n",
    "## Process all images in 'images' directory unless subfolder is included in 'ignore_directories' list         \n",
    "for directory in os.listdir('images/'):\n",
    "    if directory not in ignore_directories:\n",
    "        print()\n",
    "        print(directory)\n",
    "        process_image_directory(directory, output_directory, split)\n",
    "        \n",
    "print()\n",
    "print('------ COMPLETE ------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of input images:', IMAGE_COUNT)\n",
    "print('Total number of people saved as cropped output images:', INCLUDED_PEOPLE)\n",
    "print('Total number of people annotations in input data:', TOTAL_PEOPLE)\n",
    "print('Percent people included:', round(INCLUDED_PEOPLE/TOTAL_PEOPLE,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_images = os.listdir('A/test_' + output_directory)\n",
    "all_train_images = os.listdir('A/train_' + output_directory)\n",
    "all_val_images = os.listdir('A/val_' + output_directory)\n",
    "\n",
    "print('output_directory', output_directory)\n",
    "print('num train images',len(all_train_images))\n",
    "print('num val images',len(all_val_images))\n",
    "print('num test images',len(all_test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir('images/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "split_w28h28  -- can use 5 conv layers, and add padding to final layer\n",
    "\n",
    "Total number of input images: 3475\n",
    "Total number of people saved as cropped output images: 7965\n",
    "Total number of people annotations in input data: 19683\n",
    "Percent people included: 0.4\n",
    "\n",
    "num train images 5667\n",
    "num val images 1063\n",
    "num test images 1235\n",
    "\n",
    "\n",
    "28\n",
    "\n",
    "14\n",
    "13\n",
    "12\n",
    "11\n",
    "10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "split_w30h30  -- can use existing architecture unchanged\n",
    "\n",
    "Total number of input images: 3475\n",
    "Total number of people saved as cropped output images: 7343\n",
    "Total number of people annotations in input data: 19683\n",
    "Percent people included: 0.37\n",
    "\n",
    "30\n",
    "\n",
    "15\n",
    "14\n",
    "13\n",
    "12\n",
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_w26h26  -- can use 4 conv layers\n",
    "\n",
    "Total number of input images: 3475\n",
    "Total number of people saved as cropped output images: 8696\n",
    "Total number of people annotations in input data: 19683\n",
    "Percent people included: 0.44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_w26h70 -- can use 4 conv layers\n",
    "\n",
    "Total number of input images: 3475\n",
    "Total number of people saved as cropped output images: 7461\n",
    "Total number of people annotations in input data: 19683\n",
    "Percent people included: 0.38\n",
    "\n",
    "\n",
    "26\n",
    "\n",
    "13\n",
    "12\n",
    "11\n",
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_w25h70_v1\n",
    "\n",
    "Total number of input images: 3475\n",
    "Total number of people saved as cropped output images: 7593\n",
    "Total number of people annotations in input data: 19683\n",
    "Percent people included: 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_all_bboxvis_v2\n",
    "\n",
    "Total number of input images: 2975\n",
    "Total number of people saved as cropped output images: 6312\n",
    "Total number of people annotations in input data: 16526\n",
    "Percent people included: 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_all_bboxvis_v1\n",
    "\n",
    "Total number of input images: 2975\n",
    "Total number of people saved as cropped output images: 6131\n",
    "Total number of people annotations in input data: 16526\n",
    "Percent people included: 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_cityscapes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
